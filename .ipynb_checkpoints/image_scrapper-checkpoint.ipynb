{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coin class\n",
    "Create a coin class that will keep track of each coin's data, such as url, coin name and the images_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Class for each coin category.\n",
    "class Coin:\n",
    "    def __init__(self, coin_id, coin_name, coin_currency, coin_country):\n",
    "        self.id = coin_id\n",
    "        self.name = coin_name\n",
    "        self.currency = coin_currency\n",
    "        self.country = coin_country\n",
    "        self.currency_url = \"\" # Url of the web we are going to scrap.\n",
    "        self.image_urls = [] # All the relevant images urls ready to download.\n",
    "        self.soup = None # Our BeautifulSoup containing the html object.\n",
    "        \n",
    "    # Return the url we are going to scrap\n",
    "    def get_url (self, year=\"1990-2018\", page=0):\n",
    "        base_url = \"https://en.ucoin.net/catalog/?\" # Website url\n",
    "        country = f\"country={self.country}\" # Parameters\n",
    "        year = f\"year={year}\" # Parameters\n",
    "        page_n = str(page)\n",
    "\n",
    "        url = base_url + country + \"&\" + year + \"&type=1&page=\" + page_n\n",
    "        \n",
    "        self.currency_url = url # Update our currency_url variable.\n",
    "    \n",
    "    # Store the html of the web in a variable.\n",
    "    def get_html (self):\n",
    "        proxy_dict = {\"http\":\"http://galeria.ucm.es\"}\n",
    "        page = requests.get(self.currency_url, proxies=(proxy_dict))\n",
    "        \n",
    "        if page.status_code == 200:\n",
    "            self.soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        else:\n",
    "            raise ValueError(f\"Status code {page.status_code} for url: {self.currency_url}\") # Throw error if 404\n",
    "    \n",
    "    # Examine the html to find images of our specific coin.\n",
    "    # First we find the coin from a list of coins\n",
    "    # Second we extract a link that takes us to a galley page with lots of photos for that coin.\n",
    "    # Finally extract all the coins from the gallery.\n",
    "    def get_image_urls (self):\n",
    "        base_url = \"https://en.ucoin.net\"\n",
    "        keywords = self.name.lower().split() # Our target image keywords\n",
    "        pages = len(self.soup.select(\"div.pages a\")) # number of pages\n",
    "        all_coins = []\n",
    "        \n",
    "        # If we have multiple pages, loop through all of them.\n",
    "        if (pages != 0):\n",
    "            for ii in range(pages):\n",
    "                page_n = ii+1\n",
    "                self.get_url(page=page_n)\n",
    "                self.get_html()\n",
    "                all_coins = all_coins + self.soup.select(\"td.coin-img a\") # list of all the coins. Append coins from each page\n",
    "        else:\n",
    "            all_coins = self.soup.select(\"td.coin-img a\") # list of all the coins\n",
    "        \n",
    "        \n",
    "        all_coins_links = [a['href'] for a in all_coins] # Link to webpage with photos\n",
    "        all_coins_names = [link.split(\"/\")[-2].split(\"-\") for link in all_coins_links] # Get all the coin names in the html\n",
    "        \n",
    "        target_coin_idx = [] # indices of target coins.\n",
    "        \n",
    "        # Get the coins from the list that match our target coin name.\n",
    "        for ii, coin_name in enumerate(all_coins_names):\n",
    "            matched_keywords = [] # List with 1 if matched 0 if not matched the keyword.\n",
    "            for keyword in keywords: # For each of our keywords see if we find coins words that match.\n",
    "                matching = []\n",
    "                #if there is a word match we return 1 else 0.\n",
    "                for word in coin_name:\n",
    "                    if (keyword == word):\n",
    "                        matching.append(1)\n",
    "                    else:\n",
    "                        matching.append(0)\n",
    "                # If matching list contains a 1 we convert list to int 1, if not we convert list to int 0.       \n",
    "                matching = sorted(matching, reverse=True)\n",
    "                matching = matching[0] \n",
    "                matched_keywords.append(matching)\n",
    "                \n",
    "            matched_keywords = np.array(matched_keywords).mean() # if mean 1 it was a match. otherwise no match.\n",
    "            # Get the index of the matched link.\n",
    "            if int(matched_keywords) == 1:\n",
    "                target_coin_idx.append(ii)\n",
    "        \n",
    "        # ONLY the links of coins we want\n",
    "        target_coins_links = [base_url + coin_link for ii, coin_link in enumerate(all_coins_links) if ii in target_coin_idx]\n",
    "        \n",
    "         # Throw error if no matches were found.\n",
    "        if len(target_coins_links) == 0:\n",
    "            error = \"No matches found for {}, {}, {}, with url {}\".format(self.name, self.currency,\n",
    "                                                                                     self.country, self.currency_url)\n",
    "            print(error)\n",
    "            #raise ValueError(error)\n",
    "        \n",
    "        target_coin_link = target_coins_links[0] # Both have the same gallery. So we can choose only one.\n",
    "        gallery_base_link = base_url + \"/gallery/\" # Link with all the coins\n",
    "        target_coin_id = target_coin_link.split(\"/\")[-1]\n",
    "        target_coin_gallery_link = gallery_base_link + target_coin_id + \"&list=all\"\n",
    "\n",
    "        ### Now we have a gallery with our coins, lets get the links.\n",
    "        coin_gallery = requests.get(target_coin_gallery_link)\n",
    "        coin_gallery_html = BeautifulSoup(coin_gallery.content, 'html.parser')\n",
    "        \n",
    "        # Get the list of images ready to download\n",
    "        target_image_list = coin_gallery_html.select(\".coin-img img\")\n",
    "        target_image_list = [img['src'] for img in target_image_list]\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "            \n",
    "        # Some formatting before we go\n",
    "        for ii, image_url in enumerate(target_image_list):\n",
    "            # Delete any placeholder images\n",
    "            if image_url.split(\"/\")[-2] == \"samples\":\n",
    "                continue\n",
    "            # Some sexy formatting\n",
    "            image_url = image_url.split(\"/\")\n",
    "            image_url[-2] = image_url[-2][:-1]\n",
    "            image_url = \"/\".join(image_url)\n",
    "            \n",
    "            self.image_urls.append(image_url) # yay!!\n",
    "\n",
    "    def save_to_dir (self):\n",
    "        # Check coin id and download all the coins into that dir.\n",
    "        base_dir = \"/home/pablo/Desktop/stuff/coin_cnn/data/train\"\n",
    "        folder = \"/\" + str(self.id)\n",
    "        \n",
    "        for ii, image_url in enumerate(self.image_urls):\n",
    "            image_name = \"/{}__{}\".format(str(ii+1).zfill(2), image_url.split(\"/\")[-1])\n",
    "            image_dir = \"{}{}{}\".format(base_dir, folder, image_name)\n",
    "            \n",
    "            # Download and save the image\n",
    "            request.urlretrieve(image_url, image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might need  to scrap data from different websites. We can do this overriding some of the methods from our coin class to meet our needs. The methods we are going to need to override are:\n",
    "* ```get_url```: Each website will be different, and will take different parameters. We can adjust that for each case\n",
    "* ```get_image_urls```: when scrapping the html, we will need to scrap different tags for different sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom scrapper here. We create one for scrapping google images\n",
    "class Custom_Coin_Scrapper (Coin):\n",
    "    def __init__(self, coin_id, coin_name, coin_currency, coin_country):\n",
    "        super().__init__(coin_id, coin_name, coin_currency, coin_country) # init super class parameters.\n",
    "    \n",
    "    # Override parent get_url\n",
    "    def get_url(self):\n",
    "        base_url = \"https://www.google.com/search?tbm=isch&sa=1&ei=txYYXNaPFYO5sQH99aPwBg&q=\" # Website url. Eg. Google\n",
    "        query = f\"{self.name} coin {self.country}\".split(\" \")\n",
    "        query = \"+\".join(query)\n",
    "        \n",
    "        self.currency_url = base_url + query\n",
    "    \n",
    "    # Override get_image_urls\n",
    "    def get_image_urls (self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of Coin instances\n",
    "\n",
    "Create a list with all the necessary data for each coin to start scrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the json\n",
    "with open('cat_to_name.json', 'r') as file:\n",
    "    cat_to_name = json.load(file)\n",
    "       \n",
    "corrected_cat_2_name = [] # will contain the updated json\n",
    "\n",
    "# update indices.\n",
    "for ii, key in enumerate(cat_to_name):\n",
    "    new_key = str(ii + 1)\n",
    "    value = cat_to_name[key]\n",
    "    corrected_cat_2_name.append((new_key, value))\n",
    "        \n",
    "corrected_cat_2_name = dict(corrected_cat_2_name) # Convert list to dict\n",
    "    \n",
    "with open('cat_to_name.json', 'w') as file:   \n",
    "    json.dump(corrected_cat_2_name, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dict containing the json.\n",
    "with open('cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)\n",
    "\n",
    "# Check that it worked\n",
    "cat_to_name[\"1\"]\n",
    "len(cat_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list containing an instance of Coin for each coin.\n",
    "coin_list = []\n",
    "\n",
    "# Append coins to coin_list.\n",
    "for coin in cat_to_name.items():\n",
    "    # Get the id, name, currency and country\n",
    "    coin_id = coin[0]\n",
    "    coin_data = coin[1].split(\",\")\n",
    "    coin_name, coin_currency, coin_country = coin_data[0], coin_data[1], coin_data[2]\n",
    "    \n",
    "    # Create a Coin instance.\n",
    "    my_coin = Coin(coin_id, coin_name, coin_currency, coin_country)\n",
    "    # Push it into coin list.\n",
    "    coin_list.append(my_coin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write data to dir\n",
    "\n",
    "Time  to loop through each coin save each coin images in their folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor ii, coin in enumerate(coin_list, 1):\\n    coin_list[ii].get_url()\\n    coin_list[ii].get_html()\\n    coin_list[ii].get_image_urls()\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For testing\n",
    "'''\n",
    "for ii, coin in enumerate(coin_list, 1):\n",
    "    coin_list[ii].get_url()\n",
    "    coin_list[ii].get_html()\n",
    "    coin_list[ii].get_image_urls()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Status code 403 for url: https://en.ucoin.net/catalog/?country=australia&year=1990-2018&type=1&page=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-07b928eac311>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcoin\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoin_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcoin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcoin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcoin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcoin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-512473433d1f>\u001b[0m in \u001b[0;36mget_html\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Status code {page.status_code} for url: {self.currency_url}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Throw error if 404\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Examine the html to find images of our specific coin.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Status code 403 for url: https://en.ucoin.net/catalog/?country=australia&year=1990-2018&type=1&page=0"
     ]
    }
   ],
   "source": [
    "for coin in coin_list:\n",
    "    coin.get_url()\n",
    "    coin.get_html()\n",
    "    coin.get_image_urls()\n",
    "    coin.save_to_dir()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "l = [1,2,3,4]\n",
    "for i in l:\n",
    "    print(i)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
