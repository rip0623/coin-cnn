{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coin class\n",
    "Create a coin class that will keep track of each coin's data, such as url, coin name and the images_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Class for each coin category.\n",
    "class Coin:\n",
    "    def __init__(self, coin_id, coin_name, coin_currency, coin_country):\n",
    "        self.id = coin_id\n",
    "        self.name = coin_name\n",
    "        self.currency = coin_currency\n",
    "        self.country = coin_country\n",
    "        self.currency_url = \"\"  # Url of the web we are going to scrap.\n",
    "        self.image_urls = []  # All the relevant images urls ready to download.\n",
    "        self.soup = None\n",
    "        \n",
    "    # Return the url we are going to scrap\n",
    "    def get_url (self, year=\"1990-2018\", page=0):\n",
    "        base_url = \"https://en.ucoin.net/catalog/?\"\n",
    "        country = \"country={}\".format(self.country)\n",
    "        year = \"year={}\".format(year)\n",
    "        page_n = str(page)\n",
    "\n",
    "        url = base_url + country + \"&\" + year + \"&type=1&page=\" + page_n\n",
    "        \n",
    "        self.currency_url = url # Update our currency_url variable.\n",
    "    \n",
    "    # Store the html of the web in a variable.\n",
    "    def get_html (self):\n",
    "        headers = {'user-agent':'Mozilla/5.0'}\n",
    "        page = requests.get(self.currency_url, headers=headers)\n",
    "\n",
    "        if page.status_code == 200:\n",
    "            self.soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Status code {} for url: {}\".format(\n",
    "                    page.status_code, self.currency_url))\n",
    "    \n",
    "    # Examine the html to find images of our specific coin.\n",
    "    # First we find the coin from a list of coins\n",
    "    # Second we extract a link that takes us to a gallery page \n",
    "    # with lots of photos for that coin.\n",
    "    # Finally extract all the coins from the gallery.\n",
    "    def get_image_urls (self):\n",
    "        headers = {'user-agent':'Mozilla/5.0'}\n",
    "        base_url = \"https://en.ucoin.net\"\n",
    "        keywords = self.name.lower().split() # Our target image keywords\n",
    "        pages = len(self.soup.select(\"div.pages a\")) # number of pages\n",
    "        all_coins = []\n",
    "        \n",
    "        # If we have multiple pages, loop through all of them.\n",
    "        if (pages != 0):\n",
    "            for ii in range(pages):\n",
    "                page_n = ii+1\n",
    "                self.get_url(page=page_n)\n",
    "                self.get_html()\n",
    "                all_coins = all_coins + self.soup.select(\"td.coin-img a\")\n",
    "        else:\n",
    "            # If only one page, we take all the imgs from that page.\n",
    "            all_coins = self.soup.select(\"td.coin-img a\")\n",
    "        \n",
    "        \n",
    "        all_coins_links = [a['href'] for a in all_coins]\n",
    "        all_coins_names = [link.split(\"/\")[-2].split(\"-\") for link in all_coins_links]\n",
    "\n",
    "        target_coin_idx = []\n",
    "        \n",
    "        # Get the coins from the list that match our target coin name.\n",
    "        for ii, coin_name in enumerate(all_coins_names):\n",
    "            matched_keywords = []\n",
    "            \n",
    "            # For each of our keywords see if we find coins words that match them.\n",
    "            for keyword in keywords:\n",
    "                matching = []\n",
    "                #if there is a word match we return 1 else 0.\n",
    "                for word in coin_name:\n",
    "                    if (keyword == word):\n",
    "                        matching.append(1) # Match.\n",
    "                    else:\n",
    "                        matching.append(0) # No match.\n",
    "                        \n",
    "                matching = sorted(matching, reverse=True)\n",
    "                matching = matching[0] \n",
    "                matched_keywords.append(matching)\n",
    "                \n",
    "            matched_keywords = np.array(matched_keywords).mean() # if mean 1 it was a match. otherwise no match.\n",
    "            # Get the index of the matched link.\n",
    "            if int(matched_keywords) == 1:\n",
    "                target_coin_idx.append(ii)\n",
    "        \n",
    "        # ONLY the links of coins we want\n",
    "        target_coins_links = [base_url + coin_link \n",
    "                              for ii, coin_link in enumerate(all_coins_links) \n",
    "                              if ii in target_coin_idx]\n",
    "\n",
    "         # Throw error if no matches were found.\n",
    "        if len(target_coins_links) == 0:\n",
    "            error = \"No matches found for {}, {}, {}, with url {}\"\n",
    "            .format(self.name, self.currency, self.country, self.currency_url)\n",
    "            \n",
    "            print(error)\n",
    "        \n",
    "        target_coin_link = target_coins_links[0] # Both have the same gallery. Choose the first.\n",
    "        gallery_base_link = base_url + \"/gallery/\" # Link with all the target coins\n",
    "        target_coin_id = target_coin_link.split(\"/\")[-1]\n",
    "        target_coin_gallery_link = gallery_base_link + target_coin_id + \"&list=all\"\n",
    "        \n",
    "        # Now we have a gallery with our coins, lets get the urls.\n",
    "        coin_gallery = requests.get(target_coin_gallery_link, headers=headers)\n",
    "        coin_gallery_html = BeautifulSoup(coin_gallery.content, 'html.parser')\n",
    "        \n",
    "        # Get the list of images ready to download\n",
    "        target_image_list = coin_gallery_html.select(\".coin-img img\")\n",
    "        target_image_list = [img['src'] for img in target_image_list]\n",
    "\n",
    "            \n",
    "        # Some url formatting before we go\n",
    "        for ii, image_url in enumerate(target_image_list):\n",
    "            # Delete any placeholder images\n",
    "            if image_url.split(\"/\")[-2] == \"samples\":\n",
    "                continue\n",
    "            \n",
    "            image_url = image_url.split(\"/\")\n",
    "            image_url[-2] = image_url[-2][:-1]\n",
    "            image_url = \"/\".join(image_url)\n",
    "            \n",
    "            self.image_urls.append(image_url) # yay!!\n",
    "\n",
    "\n",
    "    def save_to_dir (self):\n",
    "        # Check coin id and download all the coins into that dir.\n",
    "        base_dir = \"/home/pablo/Desktop/stuff/coin_cnn/data/train\"\n",
    "        folder = \"/\" + str(self.id)\n",
    "        \n",
    "        for ii, image_url in enumerate(self.image_urls):\n",
    "            image_name = \"/{}__{}\".format(str(ii+1).zfill(2), image_url.split(\"/\")[-1])\n",
    "            image_dir = \"{}{}{}\".format(base_dir, folder, image_name)\n",
    "            # Download and save the image\n",
    "            request.urlretrieve(image_url, image_dir)\n",
    "        \n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll need to scrap more images from other sources. To do this I can just extend the Coin class and modify it to meet my needs. In this case I'll use the bing search api, which is a lot easier to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom scrapper here. We create one for scrapping google images\n",
    "class Coin_Bing (Coin):\n",
    "    def __init__(self, coin_id, coin_name, coin_currency, coin_country):\n",
    "        super().__init__(coin_id, coin_name, coin_currency, coin_country)\n",
    "        \n",
    "    def get_url(self):\n",
    "        base_url = \"https://duckduckgo.com/?q=\" # Website url. Eg. Google\n",
    "        parameters = \"&atb=v130-7_i&iar=images&iax=images&ia=images\"\n",
    "        query = f\"{self.name} coin {self.country}\".split(\" \")\n",
    "        query = \"+\".join(query)\n",
    "        \n",
    "        self.currency_url = base_url + query + parameters\n",
    "        print(self.currency_url)\n",
    "    def get_html(self):\n",
    "        url = self.currency_url\n",
    "        browser = webdriver.PhantomJS()\n",
    "        browser.get(url)\n",
    "        html = browser.page_source\n",
    "        \n",
    "        if page.status_code == 200:\n",
    "            self.soup = BeautifulSoup(html, 'lxml')\n",
    "        else:\n",
    "            raise ValueError(\"Status code {} for url: {}\"\n",
    "                             .format(page.status_code, self.currency_url)) # Throw error if 404\n",
    "        \n",
    "    # Override get_image_urls\n",
    "    def get_image_urls (self):\n",
    "        subscription_key = \"2c909589b185478d9b3cb4e603ac547a\"\n",
    "        assert subscription_key\n",
    "        \n",
    "        search_url = \"https://api.cognitive.microsoft.com/bing/v7.0/images/search\"\n",
    "        search_term = \"{} coin {}\".format(self.name, self.country)\n",
    "        \n",
    "        headers = {\"Ocp-Apim-Subscription-Key\" : subscription_key}\n",
    "        params  = {\"q\": search_term}\n",
    "        response = requests.get(search_url, headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        search_results = response.json()\n",
    "        self.image_urls = [img[\"thumbnailUrl\"] for img in search_results[\"value\"][:12]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of Coin instances\n",
    "\n",
    "We need to create a list of coin instances. First we need to get our cat_to_name.json into a dictionary. The dictionary will update if anything is added or removed so that there are no gaps in between indices. Once cat_to_name.json has been updated we load it into a dictionary. Finally we loop through this dictionary to create a Class instance for every coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Load the json\\nwith open('cat_to_name.json', 'r') as file:\\n    cat_to_name = json.load(file)\\n       \\ncorrected_cat_2_name = [] # will contain the updated json\\n\\n# update indices.\\nfor ii, key in enumerate(cat_to_name):\\n    new_key = str(ii + 1)\\n    value = cat_to_name[key]\\n    corrected_cat_2_name.append((new_key, value))\\n        \\ncorrected_cat_2_name = dict(corrected_cat_2_name) # Convert list to dict\\n\\n# Save the updated json.\\nwith open('cat_to_name.json', 'w') as file:   \\n    json.dump(corrected_cat_2_name, file, indent=4)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment this if we are sure that we want to update our json.\n",
    "# Only use it if changes are made manually to add or remove items.\n",
    "'''\n",
    "# Load the json\n",
    "with open('cat_to_name.json', 'r') as file:\n",
    "    cat_to_name = json.load(file)\n",
    "       \n",
    "corrected_cat_2_name = [] # will contain the updated json\n",
    "\n",
    "# update indices.\n",
    "for ii, key in enumerate(cat_to_name):\n",
    "    new_key = str(ii + 1)\n",
    "    value = cat_to_name[key]\n",
    "    corrected_cat_2_name.append((new_key, value))\n",
    "        \n",
    "corrected_cat_2_name = dict(corrected_cat_2_name) # Convert list to dict\n",
    "\n",
    "# Save the updated json.\n",
    "with open('cat_to_name.json', 'w') as file:   \n",
    "    json.dump(corrected_cat_2_name, file, indent=4)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 Cent,Australian dollar,australia'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dict containing the json.\n",
    "with open('cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)\n",
    "\n",
    "# Check that it worked\n",
    "cat_to_name[\"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list containing an instance of Coin for each coin.\n",
    "coin_list = []\n",
    "\n",
    "# Append coins to coin_list.\n",
    "for coin in cat_to_name.items():\n",
    "    # Get the id, name, currency and country\n",
    "    coin_id = coin[0]\n",
    "    coin_data = coin[1].split(\",\")\n",
    "    coin_name, coin_currency, coin_country = coin_data[0], coin_data[1], coin_data[2]\n",
    "    \n",
    "    # Create a Coin instance.\n",
    "    # my_coin = Coin(coin_id, coin_name, coin_currency, coin_country)\n",
    "    my_coin = Coin_Bing(coin_id, coin_name, coin_currency, coin_country)\n",
    "    # Push it into coin list.\n",
    "    coin_list.append(my_coin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write data to dir\n",
    "\n",
    "Time  to loop through each coin save each coin images in their folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download our images.\n",
    "for ii in range(len(coin_list)):\n",
    "    # coin_list[ii].get_url() # uncomment if using instance of Coin class (not Coin_Bing)\n",
    "    # coin_list[ii].get_html() # uncomment if using instance of Coin class (not Coin_Bing)\n",
    "    coin_list[ii].get_image_urls()\n",
    "    coin_list[ii].save_to_dir()\n",
    "    time.sleep(1) # Slow down crawling to avoid gettting banned -_-\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
